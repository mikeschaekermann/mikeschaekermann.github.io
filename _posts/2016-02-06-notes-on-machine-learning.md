---
layout: post
title: "Notes on Machine Learning"
tags:
    - python
    - notebook
--- 
These notes are a result of my preparation for a midterm exam in [Pascal
Poupart](https://cs.uwaterloo.ca/~ppoupart)'s [course on Machine
Learning](https://cs.uwaterloo.ca/~ppoupart/teaching/cs485-winter16/) at
University of Waterloo in the winter 2016 term. This post is currently a **work
in progress (as of February 6, 2016)**. 
 
## Introduction

Definition by Tom Mitchell (1988): "A computer program is said to learn from
**experience E** with respect to some class of **tasks T** and **performance
measure P** if its performance for tasks in T, as measured by P, improves with
experience E."

Inductive Learning: given a training set of examples of the form $(x, f(X))$,
return a function $h$ (**hypothesis**) that approximates $f$ (**true underlying
function**).

The quality measure for hypotheses is **generalization**. A good hypothesis will
generalize well, i.e., predict unseen examples correctly. **Ockham's razor**
demands to prefer the simplest hypothesis which is consistent with the input
data. 
 
## Decision Trees 
 
## k-Nearest Neighbors 
 
## Linear Regression 
 
## Statistical Learning 
 
## Bayesian Learning 
 
## Mixture of Gaussians 
 
## Logistic Regression 
 
## Artificial Neural Networks 
