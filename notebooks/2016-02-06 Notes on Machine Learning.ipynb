{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes are a result of my preparation for a midterm exam in [Pascal Poupart](https://cs.uwaterloo.ca/~ppoupart)'s [course on Machine Learning](https://cs.uwaterloo.ca/~ppoupart/teaching/cs485-winter16/) at University of Waterloo in the winter 2016 term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Definition by Tom Mitchell (1988): \"A computer program is said to learn from **experience E** with respect to some class of **tasks T** and **performance measure P** if its performance for tasks in T, as measured by P, improves with experience E.\"\n",
    "\n",
    "Inductive Learning: given a training set of examples of the form $(x, f(X))$, return a function $h$ (**hypothesis**) that approximates $f$ (**true underlying function**).\n",
    "\n",
    "The quality measure for hypotheses is **generalization**. A good hypothesis will generalize well, i.e., predict unseen examples correctly. **Ockham's razor** suggests to prefer the simplest hypothesis consistent with the input data.\n",
    "\n",
    "Two different types of inductive learning:\n",
    "\n",
    "* **Classification**: range/output space of $f$ is categorical (or discrete)\n",
    "* **Regression**: range/output space of $f$ is continuous\n",
    "\n",
    "**Hypothesis space $H$**: set of all possible $h$\n",
    "\n",
    "**Consistency**: $h$ is consistent if it agrees with $f$ on all examples; it is not always possible to find a consistent $h$ (e.g., due to an insufficient $H$ or noisy input data)\n",
    "\n",
    "**Realizability**: a learning problem is realizable if and only if $f$ is in $H$\n",
    "\n",
    "In general, we will observe a tradeoff between **expressiveness** (i.e., the size of $H$) and the **complexity** of finding a good $h$.\n",
    "\n",
    "**Overfitting**: given a hypothesis space $H$, a hypothesis $h \\in H$ is said to overfit the training data if there exists some alternative hypothesis $h' \\in H$ such that $h$ has smaller error than $h'$ over the training examples, but $h'$ has smaller error than $h$ over the entire distribution of instances.\n",
    "\n",
    "**$k$-fold cross-validation:** run $k$ experiments, each time putting aside $\\frac{1}{k}$ of the data to test on and, finally, compute the average accuracy of the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees (a.k.a. **C**lassification **a**nd **R**egression **T**rees [CART]) represent disjunctions (OR) of conjunctions (AND) of constraints of attribute values.\n",
    "\n",
    "Decision trees have the following **structure**:\n",
    "\n",
    "* **nodes**: attributes\n",
    "* **edges**: attribute values\n",
    "* **leaves**: classes / regression values\n",
    "\n",
    "**Quality measure** for decision trees:\n",
    "\n",
    "* small size\n",
    "* high consistency\n",
    "\n",
    "**Greedy induction** of a decision tree:\n",
    "\n",
    "* depth-first search like construction\n",
    "* a good attribute splits the examples into subsets that are ideally all from the same class or, in other words, that minimizes the residual error\n",
    "\n",
    "**Residual error for classification**:\n",
    "\n",
    "||Error Frequency|Gini Index|Entropy|\n",
    "|---|---|---|---|\n",
    "|**Definition**|$Q_\\tau=\\#\\tau-max_k\\#k$|$Q_\\tau=\\sum_kp_\\tau(k)(1-p_\\tau(k))$|$Q_t=-\\sum_kp_\\tau(k) \\ ld \\ p_\\tau(k)$|\n",
    "|**Explanation**|Number of examples in leaf $\\tau$ minus maximum number of examples in leaf $\\tau$ that belong to any class $k$|Expected misclassification when choosing the class according to $p_\\tau(k)$|Expected # of bits to encode the class of an instance chosen at random according to $p_\\tau(k)$|\n",
    "\n",
    "**Variables:**\n",
    "\n",
    "* $\\tau$: index for leaf $\\tau$\n",
    "* $k$: index for class $k$\n",
    "* $\\#\\tau$: number of examples in leaf $\\tau$\n",
    "* $\\#k$: number of examples in leaf $\\tau$ belonging to class $k$\n",
    "* $p_\\tau(k) = \\frac{\\#k}{\\#\\tau}$\n",
    "\n",
    "**Residual error for regression**:\n",
    "\n",
    "* Let $t_n = f(x_n)$ be the target for the $n^th$ example.\n",
    "* Let $y_t$ be the value returned by leaf $\\tau$.\n",
    "* Let $R_\\tau$ be the set of examples in leaf $\\tau$.\n",
    "* Euclidean error for leaf $\\tau$: $Q_\\tau=\\sum_{n \\in R_\\tau}(t_n-y_n)^2$\n",
    "\n",
    "**Choosing the best attribute** for the next decision layer in the tree:\n",
    "\n",
    "* In leaf $\\tau$, choose attribute $A$ that reduces the residual error the most when expanded:\n",
    "* $A^*=argmax_AQ_\\tau-\\sum_{a \\in A}p_\\tau(A=a)Q_{\\tau a}$, where\n",
    "  * $p_\\tau(A=a)=\\frac{\\#(A=a)}{\\#\\tau}$ denotes the proportion of examples with value $a$ (in attribute $A$) inside node $\\tau$\n",
    "  * $\\tau a$ indexes the node reached by following the edge for attribute value $a$, starting from node $\\tau$\n",
    "  \n",
    "**Techniques to avoid overfitting:**\n",
    "\n",
    "* Stop learning when the curve for testing accuracy, plotted against the tree size, goes down again; this is in practice sometimes hard to achieve because the curve might exhibit high fluctuation\n",
    "* Pruning of statistically irrelevant nodes in a bottom-up fashion\n",
    "  * Remove nodes that improve testing accuracy by less than some threshold\n",
    "  * Regularization: add a penalty term that reflects the tree complexity (e.g., $\\|T\\|=$ #leaves in the tree) and remove leaves with a negative \"regularized\" error reduction: $Q_\\tau-\\sum_{a \\in A}p_\\tau(A=a)Q_{\\tau a}-\\lambda\\|T\\|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "\n",
    "In the limit, single-attribute thresholding to construct decision trees for attributes with continouous inputs will lead to a full tree with one example per tree. Decision boundaries will always axis-aligned. A better approach without this restriction is $K$-Nearest Neighbors.\n",
    "\n",
    "**Approach:**\n",
    "\n",
    "* Let $knn(x)$ be the $K$ nearest neighbors of $x$ according to distance $d$\n",
    "* Label $y_x=mode(\\{y_x' \\ \\| \\ x' \\in knn(x)\\})$, i.e., the most frequent label among the $K$ nearest neighbors\n",
    "* $K$ controls the degree of smoothing and can be optimized using $k$-fold cross-validation (if $K$ is too small this will lead to overfitting; if $K$ is too high this will lead to underfitting)\n",
    "\n",
    "**Comparison of complexity** between decision trees and $K$-Nearest Neighbors with respect to:\n",
    "\n",
    "* $N$: size of training set\n",
    "* $M$: number of attributes\n",
    "\n",
    "||Training &nbsp;  &nbsp;  &nbsp; |Testing &nbsp;  &nbsp;  &nbsp; |\n",
    "|---|---|---|\n",
    "|**Decision Tree**|$O(M^2N)$|$O(M)$|\n",
    "|**$K$-Nearest Neighbors**|$O(MN)$|$O(MN)$|\n",
    "\n",
    "A good visualization of the decision boundaries for the two-dimensional 1-Nearest Neighbor case assuming Euclidean distance is the Voronoi diagram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.086231550409317764, 0.98264119063611655)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD7CAYAAACc26SuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9edc8acc>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Voronoi\n",
    "\n",
    "def voronoi_finite_polygons_2d(vor, radius=None):\n",
    "    \"\"\"\n",
    "    Reconstruct infinite voronoi regions in a 2D diagram to finite\n",
    "    regions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vor : Voronoi\n",
    "        Input diagram\n",
    "    radius : float, optional\n",
    "        Distance to 'points at infinity'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    regions : list of tuples\n",
    "        Indices of vertices in each revised Voronoi regions.\n",
    "    vertices : list of tuples\n",
    "        Coordinates for revised Voronoi vertices. Same as coordinates\n",
    "        of input vertices, with 'points at infinity' appended to the\n",
    "        end.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if vor.points.shape[1] != 2:\n",
    "        raise ValueError(\"Requires 2D input\")\n",
    "\n",
    "    new_regions = []\n",
    "    new_vertices = vor.vertices.tolist()\n",
    "\n",
    "    center = vor.points.mean(axis=0)\n",
    "    if radius is None:\n",
    "        radius = vor.points.ptp().max()*2\n",
    "\n",
    "    # Construct a map containing all ridges for a given point\n",
    "    all_ridges = {}\n",
    "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
    "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
    "\n",
    "    # Reconstruct infinite regions\n",
    "    for p1, region in enumerate(vor.point_region):\n",
    "        vertices = vor.regions[region]\n",
    "\n",
    "        if all([v >= 0 for v in vertices]):\n",
    "            # finite region\n",
    "            new_regions.append(vertices)\n",
    "            continue\n",
    "\n",
    "        # reconstruct a non-finite region\n",
    "        ridges = all_ridges[p1]\n",
    "        new_region = [v for v in vertices if v >= 0]\n",
    "\n",
    "        for p2, v1, v2 in ridges:\n",
    "            if v2 < 0:\n",
    "                v1, v2 = v2, v1\n",
    "            if v1 >= 0:\n",
    "                # finite ridge: already in the region\n",
    "                continue\n",
    "\n",
    "            # Compute the missing endpoint of an infinite ridge\n",
    "\n",
    "            t = vor.points[p2] - vor.points[p1] # tangent\n",
    "            t /= np.linalg.norm(t)\n",
    "            n = np.array([-t[1], t[0]])  # normal\n",
    "\n",
    "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
    "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
    "            far_point = vor.vertices[v2] + direction * radius\n",
    "\n",
    "            new_region.append(len(new_vertices))\n",
    "            new_vertices.append(far_point.tolist())\n",
    "\n",
    "        # sort region counterclockwise\n",
    "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
    "        c = vs.mean(axis=0)\n",
    "        angles = np.arctan2(vs[:,1] - c[1], vs[:,0] - c[0])\n",
    "        new_region = np.array(new_region)[np.argsort(angles)]\n",
    "\n",
    "        # finish\n",
    "        new_regions.append(new_region.tolist())\n",
    "\n",
    "    return new_regions, np.asarray(new_vertices)\n",
    "\n",
    "# make up data points\n",
    "np.random.seed(1234)\n",
    "points = np.random.rand(15, 2)\n",
    "\n",
    "# compute Voronoi tesselation\n",
    "vor = Voronoi(points)\n",
    "\n",
    "# plot\n",
    "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "\n",
    "# colorize\n",
    "for region in regions:\n",
    "    polygon = vertices[region]\n",
    "    plt.fill(*zip(*polygon), alpha=0.4)\n",
    "\n",
    "plt.plot(points[:,0], points[:,1], 'ko')\n",
    "plt.axis('equal')\n",
    "plt.xlim(vor.min_bound[0] - 0.1, vor.max_bound[0] + 0.1)\n",
    "plt.ylim(vor.min_bound[1] - 0.1, vor.max_bound[1] + 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "**Data**: $\\{(x_1, t_1),(x_2,t_2),...,(x_N,t_N)\\}$ where $x_n \\in \\mathbb{R}^D$ and $t_n \\in \\mathbb{R}$\n",
    "\n",
    "**Problem**: find linear hypothesis $h$ that maps $x$ to $t$; in other words, try to find a weight vector $w \\in \\mathbb{R}^{D+1}$ so that the error for $h(x,w)=w^T\\overline{x}$ with $\\overline{x}=\\begin{pmatrix} 1 \\\\ x \\end{pmatrix}$ over all $x$ in the dataset is minimal\n",
    "\n",
    "**Solution**: $ Aw=b $ with:\n",
    "\n",
    "* $A=\\sum_{n=1}^N\\overline{x}_n\\overline{x}_n^T$ (invertible if the training instances span $\\mathbb{R}^{D+1}$)\n",
    "* $b=\\sum_{n=1}^Nt_n\\overline{x}_n$\n",
    "\n",
    "**Tikhonov Regularization**: a technique, applied to avoid a form of overfitting where small changes to the input data lead to big changes in the learned weight vector: $(\\lambda I+A)w=b$. The greater $\\lambda$ the smaller the magnitude of $w$ will be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Learning\n",
    "\n",
    "**Idea**: learning simply reduces the uncertainty in our knowledge of the world\n",
    "\n",
    "**Marginal Probability Distribution**: specification of a probability for each event in our sample space; all probabilities must sum up to $1$\n",
    "\n",
    "**Joint Probability Distribution**: specification of probabilities for all combinations of events: $Pr(A=a\n",
    "\\wedge B=b)$ for all $a$, $b$\n",
    "\n",
    "**Marginalization** (sumout rule): $Pr(A=a)=\\sum_bPr(A=a\n",
    "\\wedge B=b)$\n",
    "\n",
    "**Conditional Probability**: fraction of worlds in which $B$ is true that also have $A$ true: $Pr(A \\mid B)=\\frac{Pr(A \\wedge B)}{Pr(B)}$; also $Pr(A \\wedge B)=Pr(A \\mid B) \\ Pr(B)$\n",
    "\n",
    "**Bayes' Rule**: $Pr(B \\mid A)=\\frac{Pr(A \\mid B) \\ Pr(B)}{Pr(A)}$\n",
    "\n",
    "**Bayesian Inference**: $P(H \\mid e)=kP(e \\mid H) \\ P(H)$\n",
    "\n",
    "* $H$: hypothesis\n",
    "* $e$: evidence\n",
    "* $P(H)$: **prior** probability of $H$\n",
    "* $P(e \\mid H)=\\prod_nP(e_n \\mid H)$: **likelihood** of observing $e$, given $H$\n",
    "* $P(H \\mid e)$: **posterior** probability of $H$, given $e$\n",
    "* $k$: normalizing factor, applied so that all posteriors sum up to $1$\n",
    "\n",
    "**Bayesian Prediction**: $P(X \\mid e)=\\sum_iP(X \\mid h_i) \\ P(h_i \\mid e)$\n",
    "\n",
    "**Properties of Bayesian Learning**: optimal and not prone to overfitting, but potentially intractable if the hypothesis space is large\n",
    "\n",
    "**Approximations of Bayesian Learning**:\n",
    "\n",
    "* **Maximum a Posteriori (MAP)**:\n",
    "  * making predictions, based on the most probable hypothesis $h_{MAP}=argmax_{h_i}P(h_i \\mid e)$\n",
    "  * less accurate than Bayesian prediction, but both converge in accuracy as data increases\n",
    "  * controlled overfitting (prior can be used to penalize complex hypotheses)\n",
    "  * MAP for linear regression leads to regularized least square problem\n",
    "* **Maximum Likelihood (ML)**:\n",
    "  * MAP with uniform prior: $h_{ML}=argmax_{h_i}P(e \\mid h_i)$\n",
    "  * less accurate than Bayesian and MAP prediction, but all three converge in accuracy as data increases\n",
    "  * prone to overfitting\n",
    "  * ML for linear regression leads to non-regularized least square problem\n",
    "  \n",
    "**Bias-Variance Decomposition** for linear regression: $expected \\ loss = (bias)^2 + variance + noise$\n",
    "\n",
    "* bias increases as regularization parameter $\\lambda$ increases\n",
    "* variance decreases as regularization parameter $\\lambda$ increases\n",
    "* noise is constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture of Gaussians\n",
    "\n",
    "**Purpose**: linear classification technique\n",
    "\n",
    "**Assumptions**:\n",
    "\n",
    "* $Pr(C=c_k)=\\pi_k$: prior probability of class $k$\n",
    "* $Pr(x \\mid C=c_k) \\propto e^{-0.5(x-\\mu_k)^T\\Sigma^{-1}(x-\\mu_k)}$: likelihood of data point $x$, given class $k$, is a Gaussian distribution with the same covariance matrix $\\Sigma$ for all classes\n",
    "* $Pr(C=c_k \\mid x)=kPr(x \\mid C=c_k)Pr(C=c_k)$: posterior probability of class $k$, given data point $x$\n",
    "\n",
    "**For two classes $A$ and $B$** (using sigmoid):\n",
    "\n",
    "$Pr(C=A)=\\sigma(w^Tx+w_0)$ where\n",
    "\n",
    "* $\\sigma(a) = \\frac{1}{1+e^{-a}}$ (sigmoid function)\n",
    "* $w=\\Sigma^{-1}(\\mu_A-\\mu_B)$\n",
    "* $w_0=-0.5\\mu_A^T\\Sigma^{-1}\\mu_A+0.5\\mu_B^T\\Sigma^{-1}\\mu_B+ln\\frac{\\pi_A}{\\pi_B}$\n",
    "* $\\pi_k:$ fraction of training examples that belong to class $k$ (via maximum likelihood)\n",
    "* $\\mu_k:$ empirical mean of all training examples that belong to class $k$ (via maximum likelihood)\n",
    "* $\\Sigma=\\frac{S_A+S_B}{N}$: normalized sum of covariance matrices (via maximum likelihood)\n",
    "* $S_k=\\sum_{n \\in c_k}(x_n-\\mu_k)(x_n-\\mu_k)^T$\n",
    "\n",
    "**For multiple classes** (using softmax):\n",
    "\n",
    "$Pr(C=c_k \\mid x)=\\frac{e^{w_k^T\\overline{x}}}{\\sum_je^{w_j^T\\overline{x}}}$\n",
    "\n",
    "**Prediction**: best class $k^*=argmax_kPr(c_k \\mid x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "**Purpose**: linear classification technique (can be viewed as a regression where the goal is to estimate a posterior probability which is continuous)\n",
    "\n",
    "**Assumption**: $Pr(x \\mid C=c_k)$ are members of the exponential family: $Pr(x \\mid \\Theta_k)=exp(\\Theta_k^TT(x)-A(\\Theta_k)+B(x))$\n",
    "\n",
    "**Derivation**: $Pr(C=c_k \\mid x)=\\sigma(w^T\\overline{x})$: the posterior probability of class $k$ is a sigmoid logistic linear in $x$ (or softmax linear in $x$ for more than two classes)\n",
    "\n",
    "**Idea**: learning $Pr(C=c_k \\mid x)$ directly by maximum likelihood\n",
    "\n",
    "**Implementation for binary classification**:\n",
    "\n",
    "* $y \\in \\{0,1\\}$\n",
    "* $w^*=argmax_w\\prod_n\\sigma(w^T\\overline{x})^{y_n}(1-\\sigma(w^T\\overline{x}))^{1-y_n}$\n",
    "* $ \\ \\ \\ \\ \\ =argmin_w-\\sum_nln(\\sigma(w^T\\overline{x}))+(1-y_n)ln(1-\\sigma(w^T\\overline{x}))$\n",
    "* Derivate $\\nabla L(w)=\\sum_n(\\sigma(w^T\\overline{x})-y_n)\\overline{x}_n$\n",
    "* Solve derivative iteratively for $0$ using Newton's method: $w_{i+1}=w_i-H^{-1}\\nabla L(w)$ where\n",
    "  * $H=\\overline{X}R\\overline{X}^T$ is the Hessian matrix\n",
    "  * $R$ is a diagonal matrix with entries of $\\sigma_n(1-\\sigma_n)$\n",
    "  * $\\sigma_n=\\sigma(w_i^T\\overline{x}_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Models\n",
    "\n",
    "**Purpose**: non-linear classification/regression\n",
    "\n",
    "**Idea**: map inputs to a different space using a set of basis functions and do linear classification/regression in that space\n",
    "\n",
    "**Common basis functions**:\n",
    "\n",
    "* Polynomial: $\\theta_j(x)=x^j$\n",
    "* Gaussian: $\\theta_j(x)=e^{-\\frac{(x-\\mu_j)^2}{2s^2}}$\n",
    "* Sigmoid: $\\theta_j(x)=\\sigma(\\frac{(x-\\mu_j)^2}{s})$\n",
    "* Fourier, Wavelets etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "\n",
    "**Purpose**: non-linear classification/regression\n",
    "\n",
    "**Idea**: network of units similar to neurons in a human brain\n",
    "\n",
    "**Implementation**: numerical output of unit $j$, $h(a_j)$ where\n",
    "\n",
    "* $a_j=\\sum_{i}W_{ji}x_i+w_0=W_j\\overline{x}$\n",
    "* $W_{ji}$ denotes the strength of the link from unit $i$ to unit $j$\n",
    "* $h(x)$ is the activation function (e.g., threshold, sigmoid, Gaussian, hyperbolic tangent, identity)\n",
    "\n",
    "**Structures**:\n",
    "\n",
    "* feed-forward network (directed acyclic graph)\n",
    "* recurrent network (directed cyclic graph)\n",
    "\n",
    "**Perceptron**: single-layer feed-forward network\n",
    "\n",
    "* **Threshold Perceptron Learning**:\n",
    "  * done separately for each unit $j$\n",
    "  * for each $(x,y)$ pair, correct weight $W_{ji}$ if incorrect output is produced:\n",
    "    * if produced is $0$ instead of $1$: $W_{ji}=W_{ji}+x_i$\n",
    "    * if produced is $1$ instead of $0$: $W_{ji}=W_{ji}-x_i$\n",
    "  * convergence if and only if the dataset is linearly separable\n",
    "* **Sigmoid Perceptron Learning**:\n",
    "  * same hypothesis space as logistic regression\n",
    "\n",
    "**Multi-layer neural networks**: flexible non-linear models by learning non-linear basis functions\n",
    "\n",
    "* examples of 2-layer feed-forward networks:\n",
    "  * $h_1$ non-linear and $h_2$ **sigmoid**: non-linear **classification**\n",
    "  * $h_1$ non-linear and $h_2$ **identity**: non-linear **regression**\n",
    "\n",
    "**Back Propagation**:\n",
    "\n",
    "* Purpose: learning by iteratively adjusting network's weights to minimze output error\n",
    "* Two phases:\n",
    "  * Forward phase: compute output $z_j$ for each unit $j$\n",
    "  * Backward phase: comput delta $\\delta_j$ at each unit $j$:\n",
    "    * if $j$ is an output unit: $\\delta_j=h'(a_j)(y_j-z_j)$\n",
    "    * if $j$ is a hidden unit: $\\delta_j=h'(a_j)\\sum_kw_{kj}\\delta_k$ (recursion)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
